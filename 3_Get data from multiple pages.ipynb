{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb37a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba56fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(soup):\n",
    "    try:\n",
    "        sub_page = requests.get(soup, headers=headers)\n",
    "        soup2 = BeautifulSoup(sub_page.content, \"html.parser\")\n",
    "        return \"\"\n",
    "\n",
    "    except:\n",
    "        return \"bad link\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a92de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\", attrs = {'id': 'productTitle'}).text.strip()\n",
    "    except:\n",
    "        return \"Error, check the link\"\n",
    "        \n",
    "        \n",
    "def get_price(soup):\n",
    "    try:    \n",
    "        return float(soup.find(\"span\", attrs = {'class': 'a-price a-text-price a-size-medium apexPriceToPay'}).find(\"span\", attrs = {'class': 'a-offscreen'}).text.strip(\"$ \"))  \n",
    "    except:    \n",
    "        try:\n",
    "            return float(soup.find(\"span\", attrs = {'class': 'a-price aok-align-center'}).find(\"span\", attrs = {'class': 'a-offscreen'}).text.strip(\"$ \"))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        return (soup.find(\"i\", attrs = {'class': 'a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big'}).text.strip(\"out of 5 stars\"))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_stock(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\", attrs = {'class': 'a-size-medium a-color-success'}).text.strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_brand(soup):\n",
    "    if \"head\" in get_title(soup).lower():\n",
    "        return \"Head\"\n",
    "    elif \"wilson\" in get_title(soup).lower():\n",
    "        return \"Wilson\"\n",
    "    elif \"babolat\" in get_title(soup).lower():\n",
    "        return \"Babolat\"\n",
    "    elif \"yonex\" in get_title(soup).lower():\n",
    "        return \"Yonex\"\n",
    "    elif get_title(soup) == \"\":\n",
    "        return \"Title not found\"\n",
    "    else:\n",
    "        return \"Other Brand\"\n",
    "\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        return (soup.find(\"a\", attrs = {'id': 'acrCustomerReviewLink'}).text.strip(\" ratings\"))\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "786668d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_list_to_csv(data_list, filename):\n",
    "    # Extract column names from the keys of the first dictionary\n",
    "    fieldnames = list(data_list[0].keys())\n",
    "\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header row\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write each dictionary as a row in the CSV file\n",
    "        writer.writerows(data_list)\n",
    "\n",
    "    print(f\"Data stored in '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae04102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the website and pull data\n",
    "# http://httpbin.org/get to get the user agent\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "#main_link = input(\"Enter url: \")\n",
    "\n",
    "product_hrefs = []\n",
    "\n",
    "#Pulling product page urls from the first 5 pages and storing them in product_hrefs\n",
    "\n",
    "for x in range(1,6):\n",
    "    URL = f\"https://www.amazon.ca/s?k=tennis+racket&i=sporting&rh=n%3A2242989011%2Cp_89%3ABabolat%7CHEAD%7CWilson%7CYONEX&dc&qid=1687390998&rnid=7590290011&ref=sr_nr_p_89_5&ds=v1%3A6ZIJSU2BVM2Kmnzns%2Bglmsz2vut1AmfrJyCEboObpd8&page={x}\"\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    main_soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    product_hrefs = product_hrefs + (main_soup.find_all(\"a\", attrs={'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'}))\n",
    "    \n",
    "product_links = []\n",
    "\n",
    "# Converitng collected hrefs into usable links and storing in product_links list\n",
    "\n",
    "for product_href in product_hrefs:\n",
    "    product_links.append(\"https://amazon.ca\" + product_href.get('href'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee8562be",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "\n",
    "# going through each product link and getting product information and storing it as a list of dictionary for each product\n",
    "\n",
    "for link in product_links:\n",
    "    sub_page = requests.get(link, headers=headers)\n",
    "    soup2 = BeautifulSoup(sub_page.content, \"html.parser\")\n",
    "    product = {\n",
    "    \"title\": get_title(soup2),\n",
    "    \"brand\": get_brand(soup2),\n",
    "    \"price\": get_price(soup2),\n",
    "    \"rating\": get_rating(soup2),\n",
    "    \"total reviews\": get_review_count(soup2),\n",
    "    \"stock\": get_stock(soup2),\n",
    "    \"link\": link}\n",
    "    products.append(product)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a8f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_products = []\n",
    "\n",
    "# filtering products to remove items that are NOT racquets or are racquets for kids\n",
    "\n",
    "for product in products:\n",
    "    if any(keyword in product[\"title\"].lower() for keyword in [\"junior\", \"youth\", \"child\", \"kid\"]) or (0 < product[\"price\"] < 25):\n",
    "        continue\n",
    "    else:\n",
    "        filtered_products.append(product)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d45207b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'Output_3_1_Tennis_Racquets_Filtered.csv'.\n"
     ]
    }
   ],
   "source": [
    "write_dict_list_to_csv(filtered_products, \"Output_3_1_Tennis_Racquets_Filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ad08fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in 'Output_3_2_Tennis_Racquets_all.csv'.\n"
     ]
    }
   ],
   "source": [
    "write_dict_list_to_csv(products, \"Output_3_2_Tennis_Racquets_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a4014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b81e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
